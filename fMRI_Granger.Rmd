---
title: "fMRI"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
author: "Ludovica Nucci (Start 23/03/2020; language: it)"
output: 
  html_document: 
    fig_height: 7
    fig_width: 10
    highlight: tango
    theme: cerulean
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r preamble, message=FALSE, warning=FALSE}
library(rstudioapi)
cat("\f")
rm(list=ls())
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )); print(getwd())

library(sysid)
library(MTS)
```

#Dati
La risonanza magnetica acquisisce un'immagine ogni 0.72s (ts=0.72 tempo in secondi tra un'osservazione e la successiva), la prima immagine è acquisita al tempo 0; lf, rf, lh, rh e t sono matrici che contengono sulla prima colonna i tempi di inizio dei tasks motori (in secondi), ciascun task ha una durata pari a  12s.
X è la serie temporale degli stimoli che vale 0 nei tempi a riposo ed 1 durante i tasks motori (aggiungiamo +1 a tutti i tempi per tenere conto del fatto che la prima immagine è acquisita al tempo 0).

La matrice dataMatrix contiene per colonna le serie temporali dei voxel del soggetto in esame.
```{r DATI}
lf=read.table("./tfMRI_MOTOR_LR/lf_100206.txt", sep="\t") 
rf=read.table("./tfMRI_MOTOR_LR/rf_100206.txt", sep="\t")
rh=read.table("./tfMRI_MOTOR_LR/rh_100206.txt", sep="\t")
lh=read.table("./tfMRI_MOTOR_LR/lh_100206.txt", sep="\t")
t=read.table("./tfMRI_MOTOR_LR/t_100206.txt", sep="\t")
#la risonanza magnetica acquisisce un'immagine ogni 0.72s, 12s è la durata di ciascun task motorio, la prima immagine è acquisita al tempo 0
# 1=motion 0=resting 
X=rep(0,284)#Serie temporale degli stimoli
X[(ceiling(lf[1,1]/0.72)+1) : (floor((lf[1,1]+12)/0.72)+1)]=1
X[(ceiling(lf[2,1]/0.72)+1) : (floor((lf[2,1]+12)/0.72)+1)]=1
X[(ceiling(rf[1,1]/0.72)+1) : (floor((rf[1,1]+12)/0.72)+1)]=1
X[(ceiling(rf[2,1]/0.72)+1) : (floor((rf[2,1]+12)/0.72)+1)]=1
X[(ceiling(rh[1,1]/0.72)+1) : (floor((rh[1,1]+12)/0.72)+1)]=1
X[(ceiling(rh[2,1]/0.72)+1) : (floor((rh[2,1]+12)/0.72)+1)]=1
X[(ceiling(lh[1,1]/0.72)+1) : (floor((lh[1,1]+12)/0.72)+1)]=1
X[(ceiling(lh[2,1]/0.72)+1) : (floor((lh[2,1]+12)/0.72)+1)]=1
X[(ceiling(t[1,1]/0.72)+1) : (floor((t[1,1]+12)/0.72)+1)]=1
X[(ceiling(t[2,1]/0.72)+1) : (floor((t[2,1]+12)/0.72)+1)]=1

ts=0.72 #Sampling time interval
out_order=2 #Number of output delays
in_order=6 #Number of input delays
max_order=max(out_order, in_order)
comp_nr = 4 #Number of connectivity components
act_thresh=0.07 #Activation threshold

load("~/TESI/tfMRI_MOTOR_LR/sub_100206.RData")
dataMatrix=ar_100206[-c(1,2,3),] 
#la matrice dataMatrix contiene per colonna le serie temporali dei voxel del soggetto 100206
```

#Activation_base

Dati in ingresso: 
X= extra input (la serie temporale degli stimoli) 
Y= output (serie temporale del voxel in esame centrata)
in_order= numero di lag per la X
out_order= numero di lag per la Y
ts= tempo in secondi tra un'osservazione e la successiva (0.72s)

Passo 1: MODELLO AR ( Y )
Calibriamo un modello autoregressivo di ordine out_order per la serie temporale Y.
La struttura del modello è data dalla seguente equazione:
$Y[t]=a_1*Y[t-1]+a_2*Y[t-2]+\dots+a_{\mbox{out_order}}*Y[t-\mbox{out_order}]+w_t$ per $t > \mbox{out_order}$
dove $\{w_t\}$ è un white noise.
Utilizziamo il comando ar in cui specifichiamo la serie temporale Y e l'ordine del modello order.max = out_order.

Passo 2: MODELLO ARX (X-Y)
Il modello ARX di ordini (out_order, in_order, 1) è un modello autoregressivo in cui è presente una variabile esogena; è della forma
$Y[t]+a_1*Y[t-1]+a_2*Y[t-2]+...+a_{\mbox{out_order}}*Y[t-\mbox{out_order}]=b_1*X[t-1]+b_2*X[t-2]+...+b_{\mbox{in_order}}*X[t-\mbox{in_order}] +w_t$ per $t> \max(\mbox{out_order}, \mbox{in_order})$
dove $\{w_t\}$ è un white noise.
Utilizziamo il comando arx; in ingresso abbiamo:
-data, un oggetto di classe idframe contenente le coppie input-output X[t] Y[t];
-order=c(out_order, in_order, 1) dove out_order è l'ordine associato all'output Y, in_order è l'ordine associato all'extra input X e l'ultimo termine è l'input-output delay, ovvero il numero di tempi necessari affinché l'input influenzi l'output (fissiamo il valore 1);
-lambda è il fattore di regolarizzazione, scegliamo lambda=0 per annullare l'effetto della regolarizzazione.
Il comando ar non prevede regolarizzazione, i coefficienti del modello sono determinati con la stima dei minimi quadrati, per questo eliminiamo la regolarizzazione in modo da avere modelli coerenti. (È giusto? nell'help dice che il valore di default è lambda=0.1)

Dato in uscita:
-RSS_AR è la somma dei quadrati dei residui del modello AR
-RSS_ARX è la somma dei quadrati dei residui del modello ARX
È possibile che i modelli AR e ARX abbiano un diverso numero di residui in base alla scelta di in_order e out_order, per questo consideriamo soltanto i tempi a partire da max_order +1 in poi. (il problema non esisterebbe se scegliessimo in_order minore o uguale di out_order ma è limitante, così è ok?)
- $1-\frac{\mbox{RSS_ARX}}{\mbox{RSS_AR}}$ è la proporzione di varianza spiegata dalla serie temporale degli stimoli X.
Fissiamo una soglia act_thresh (ho messo 0.07 solo per avere un numero ragionevole di voxel attivati); se per un dato voxel risulta $1-\frac{\mbox{RSS_ARX}}{\mbox{RSS_AR}} > \mbox{act_thresh}$ significa che la serie temporale degli stimoli accresce la conoscenza della serie temporale del voxel, ovvero Granger-causa la serie temporale del voxel.

Per ciascun voxel consideriamo la sua serie temporale, gli sottraiamo la media e calcoliamo la funzione Activation_base. (Consideriamo serie temporali centrate in modo da non dover mettere un termine costante nei modelli AR e ARX?)
I voxel per cui act[i]>act_thresh sono quelli che si sono attivati in risposta agli stimoli dei tasks motori secondo il modello Granger (attivazioni secondarie).

```{r ACTIVATION DETECTION THROUGH GRANGER CAUSALITY}
#(~17 min)

Activation_base <- function(X, Y, in_order, out_order, ts){
  #AR model for the time serie Y
  ARmodel=ar(Y, aic = FALSE, order.max = out_order) #Y[n]=ARmodel$ar[1]*Y[n-1]+ARmodel$ar[2]*Y[n-2]+ARmodel$resid[n]
  RSS_AR=sum(ARmodel$resid[(max_order+1):284]^2)
  #ARX model for the pair [X Y]
  data=idframe(Y, X , Ts = ts, start = 0, unit = "seconds")
  ARXmodel=arx(data, order = c(out_order, in_order, 1), lambda = 0, intNoise = FALSE, fixed = NULL )
  res_ARX= Y[(max_order+1):284]-ARXmodel$fitted.values[(max_order+1):284]
  #ARXmodel$fitted.values[n]=-(ARXmodel$sys$A[2]*Y[n-1]+ARXmodel$sys$A[3]*Y[n-2])+ARXmodel$sys$B[1]*st[n-1]+ARXmodel$sys$B[2]*st[n-2]
  RSS_ARX=sum(res_ARX^2)
  
  1-(RSS_ARX/RSS_AR)
}
  
act=rep(0, dim(dataMatrix)[2])
for (i in 1:dim(dataMatrix)[2]) {
    Y_temp = dataMatrix[,i]
    Y = Y_temp - mean(Y_temp)
    act[i]=Activation_base(X, Y, in_order, out_order, ts)
}

#save(act, file = "./tfMRI_MOTOR_LR/act_in=6_out=2_lambda=0_R^2.RData")   
#load("~/TESI/tfMRI_MOTOR_LR/act_in=6_out=2_lambda=0_R^2.RData")
```

Per ciascun voxel attivato, cerchiamo tra gli altri voxel attivati quali sono quelli che hanno più alta causalità rispetto a lui.

Prendiamo in esame l'i_esimo voxel attivato ed eseguiamo un ciclo for che scorre su tutti gli altri voxel attivati; ad ogni passo j calcoliamo la funzione Activation_base, con extra input la serie del j_esimo voxel attivato ed output la serie del voxel i_esimo centrata. (nel matlab non viene centrata ma credo che sia giusto farlo ??)
Raccogliamo i risultati nella matrice conn_temp; è una matrice (# di voxel attivati) x 2 in cui alla riga j_esima abbiamo, nella prima colonna il valore di Granger tra il j_esimo voxel e l'i_esimo, nella seconda il numero della colonna che il j_esimo voxel attivato occupa in dataMatrix.
Riordiniamo le righe della matrice in ordine decrescente rispetto alla prima colonna, in modo da avere nelle prime righe i voxel che hanno causalità più alta rispetto al voxel in esame.

Salviamo le informazioni nelle matrici (comp_nr+1) x (# voxel attivati) CONN_matr e CONN_matr_gr.
-CONN_matr: nella i_esima colonna ha il numero della colonna che l' i_esimo voxel attivato occupa in dataMatrix e i numeri delle colonne che occupano i comp_nr voxel attivati con causalità più alta rispetto all'i_esimo.
-CONN_matr_gr: nella i_esima colonna ha il numero della colonna che l' i_esimo voxel attivato occupa in dataMatrix e i valori di Granger tra i comp_nr voxel attivati che hanno causalità più alta rispetto all'i_esimo voxel e l'i_esimo voxel.

```{r DETECTION OF MAXIMALLY CAUSED VOXEL WITH RESPECT TO EACH VOXEL}

CONN_matr=c()
CONN_matr_gr=c()
for (i in 1:length(which(act>act_thresh))) {
  act_idx=which(act>act_thresh)[-i]
  conn_temp = c()
  Y_temp=dataMatrix[,which(act>act_thresh)[i]]
  Y = Y_temp - mean(Y_temp)
  for (j in 1:length(act_idx)) {
     conn_temp=rbind(conn_temp, c(Activation_base(dataMatrix[,act_idx[j]], Y, in_order, out_order, ts), act_idx[j]))
     #conn_temp è una matrice "# di voxel attivati" x 2 -> alla riga j_esima abbiamo, nella prima colonna il valore di Granger tra il j_esimo voxel e l'i_esimo, nella seconda il numero della colonna che il j_esimo voxel attivato occupa in dataMatrix
  }
  ord=order(conn_temp[,1], decreasing = TRUE)
  conn_temp=conn_temp[ord,] #nelle prime righe i voxel che hanno causalità più alta rispetto al voxel i_esimo
  conn_temp2 = conn_temp[1:comp_nr,2]
  CONN_matr = cbind( CONN_matr, c(which(act>act_thresh)[i], conn_temp2))
  #CONN_matr è la matrice (comp_nr+1) x (# voxel attivati) che nella i_esima colonna ha il numero della colonna che l' i_esimo voxel attivato occupa in dataMatrix e i numeri delle colonne che occupano gli nr_comp voxel attivati con causalità più alta rispetto all'i_esimo
  conn_temp1 = conn_temp[1:comp_nr,1]
  CONN_matr_gr = cbind(CONN_matr_gr, c(which(act>act_thresh)[i], conn_temp1))
  #CONN_matr_gr è la matrice (comp_nr+1) x (# voxel attivati) che nella i_esima colonna ha il numero della colonna che l'i_esimo voxel attivato occupa in dataMatrix e i valori di Granger tra "i comp_nr voxel attivati che hanno causalità più alta rispetto all'i_esimo voxel" e "l'i_esimo voxel"
}

#save(CONN_matr, file = "./tfMRI_MOTOR_LR/CONN_matr_in=6_out=2_.99_lambda=0_R^2_mean.RData")   
#save(CONN_matr_gr, file = "./tfMRI_MOTOR_LR/CONN_matr_gr_in=6_out=2_.99_lambda=0_R^2_mean.RData")  
#load("~/TESI/tfMRI_MOTOR_LR/CONN_matr_in=6_out=2_.99_lambda=0_R^2_mean.RData")
#load("~/TESI/tfMRI_MOTOR_LR/CONN_matr_gr_in=6_out=2_.99_lambda=0_R^2_mean.RData")
```

#Activation

Dati in ingresso: 
X= extra input (la serie temporale degli stimoli)
CONN= extra input (matrice 284 x comp_nr che contiene per colonna le serie temporali dei comp_nr voxel con più alta causalità rispetto a quello in esame)
Y= output (serie temporale del voxel in esame centrata)
in_order= numero di lag per gli extra input
out_order= numero di lag per la Y

Passo 1: MODELLI ARX Multiple Input-Single Output (CONN-Y) e (X,CONN-Y)
Calibriamo i modelli ARX MISO di ordini (out_order, in_order, 1) descritti dalle seguenti equazioni:

$Y[t]= a_1*Y[t-1]+a_2*Y[t-2]+\dots+a_{\mbox{out_order}}*Y[t-\mbox{out_order}]+{c^1}_1*\mbox{CONN}[t-1,1]+{c^2}_1*\mbox{CONN}[t-1,2]+\dots+{c^{\mbox{comp_nr}}}_1*\mbox{CONN}[t-1,\mbox{comp_nr}]+{c^1}_2*\mbox{CONN}[t-2,1]+{c^2}_2*\mbox{CONN}[t-2,2]+\dots+{c^{\mbox{comp_nr}}}_2*\mbox{CONN}[t-2,\mbox_{comp_nr}]+{c^1}_{\mbox{in_order}}*\mbox{CONN}[t-\mbox{in_order},1]+{c^2}_{\mbox{in_order}}*\mbox{CONN}[t-\mbox{in_order},2]+\dots+{c^{\mbox{comp_nr}}_{\mbox{in_order}}*\mbox{CONN}[t-\mbox{in_order},\mbox{comp_nr}]+w_t$ 
per $t> \max(\mbox{out_order},\mbox{in_order})$

$Y[t]= a_1*Y[t-1]+a_2*Y[t-2]+\dots+a_{\mbox{out_order}}*Y[t-\mbox{out_order}]+b_1*X[t-1]+b_2*X[t-2]+...+b_{\mbox{in_order}}*X[t-\mbox{in_order}] +{c^1}_1*\mbox{CONN}[t-1,1]+{c^2}_1*\mbox{CONN}[t-1,2]+\dots+{c^{\mbox{comp_nr}}}_1*\mbox{CONN}[t-1,\mbox{comp_nr}]+{c^1}_2*\mbox{CONN}[t-2,1]+{c^2}_2*\mbox{CONN}[t-2,2]+\dots+{c^{\mbox{comp_nr}}}_2*\mbox{CONN}[t-2,\mbox{comp_nr}]+{c^1}_{\mbox{in_order}}*\mbox{CONN}[t-\mbox{in_order},1]+{c^2}_{\mbox{in_order}}*\mbox{CONN}[t-\mbox{in_order},2]+\dots+{c^{\mbox{comp_nr}}_{\mbox{in_order}}*\mbox{CONN}[t-\mbox{in_order},\mbox{comp_nr}]+w_t$ 
per $t> \max(\mbox{out_order},\mbox{in_order})$
dove $\{w_t\}$ è un white noise.

Utilizziamo il comando VARX, che è l'estensione del comando arx al caso di serie temporali multivariate, ci consente quindi di lavorare con multipli extra input.
In ingresso abbiamo:
- la serie temporale Y ed il relativo ordine autoregressivo;
- xt è la matrice delle serie temporali in input ed m è il relativo ordine autoregressivo.
La funzione non prevede la possibilità di un input_output delay. 
Per risolvere il problema prendiamo la serie Y dal tempo 2 a 284, le serie in input dal tempo 1 a 283 e m= in_order-1 (perché c'è anche il lag 0) in modo da creare il ritardo voluto. 
(ha senso fare così? in questo modo perdiamo in entrambi i casi il primo residuo però i modelli sono più simili ai precedenti anche se non sono sicura che sia davvero necessario)

Dato in uscita:
L'output è analogo a quello della funzione Activation_base ed è la proporzione di varianza spiegata dalla serie degli stimoli.
-RSS_red è la somma dei quadrati dei residui del modello (CONN-Y)
-RSS_full è la somma dei quadrati dei residui del modello (X,CONN-Y)

Per ogni voxel attivato secondo il modello Granger calcoliamo la funzione Activation e riportiamo il risultato in act_CONN.
I voxel per cui act_CONN[i]>act_thresh sono quelli che si sono attivati in risposta agli stimoli dei tasks motori secondo il modello che tiene conto della connettività tra i voxel (attivazioni primarie).
(act_CONN nella versione precedente aveva lunghezza pari al numero di voxel attivati, adesso è un vettore di lunghezza uguale al numero di voxel, con elementi diversi da zero in corrispondenza delle posizioni dei voxel attivati, in questo modo è immediato risalire alla colonna del voxel attivato in dataMatrix, prima era più macchinoso)

```{r ACTIVATION DETECTION THROUGH CONNECTIVITY}

Activation<- function(X, Y, CONN, in_order, out_order){
  #ARX model for the time series Y CONN
  ARXmodel_red=VARX(Y[2:284], out_order, xt = CONN[1:283,], m = (in_order-1), include.mean = F, fixed = NULL)
  #Y[n]-(ARXmodel_red$coef[1]*Y[n-1]+ARXmodel_red$coef[2]*Y[n-2]+ARXmodel_red$coef[3]*CONN[n-1,1]+ARXmodel_red$coef[4]*CONN[n-1,2]+ARXmodel_red$coef[5]*CONN[n-1,3]+ARXmodel_red$coef[6]*CONN[n-1,4]+ARXmodel_red$coef[7]*CONN[n-2,1]+ARXmodel_red$coef[8]*CONN[n-2,2]+ARXmodel_red$coef[9]*CONN[n-2,3]+ARXmodel_red$coef[10]*CONN[n-2,4])=ARXmodel_red$residuals[n-3] n>3
  RSS_red=sum(ARXmodel_red$residuals^2)
  #ARX model for [X CONN Y]
  ARXmodel_full=VARX(Y[2:284], out_order, xt = cbind(X[1:283],CONN[1:283,]), m = (in_order-1), include.mean = F, fixed= NULL)
  RSS_full=sum(ARXmodel_full$residuals^2)
  1-(RSS_full/RSS_red) 
}

act_CONN = rep(0,dim(dataMatrix)[2])
for (i in 1:length(which(act>act_thresh))){
  Y_temp = dataMatrix[,which(act>act_thresh)[i]]
  Y = Y_temp - mean(Y_temp)
  CONN=dataMatrix[, CONN_matr[-1,i]] #CONN è una matrice 284 x comp_nr che contiene per colonna le serie temporali dei comp_nr voxel con più alta causalità rispetto a quello in esame
  act_CONN[which(act>act_thresh)[i]]=Activation(X, Y, CONN, in_order, out_order)
}

#length(which(act > act_thresh)) 1097
#length(which(act_CONN> act_thresh)) 385
```